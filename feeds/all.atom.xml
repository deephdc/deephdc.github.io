<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>DEEP Open Catalog</title><link href="https://marketplace.deep-hybrid-datacloud.eu/" rel="alternate"></link><link href="https://marketplace.deep-hybrid-datacloud.eu/feeds/all.atom.xml" rel="self"></link><id>https://marketplace.deep-hybrid-datacloud.eu/</id><updated>2020-06-02T11:08:18+02:00</updated><entry><title>Artistic style transfer</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-neural-transfer.html" rel="alternate"></link><published>2020-04-09T00:00:00+02:00</published><updated>2020-06-02T11:08:15+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2020-04-09:/modules/deep-oc-neural-transfer.html</id><summary type="html">&lt;p&gt;A module to apply artistic style transfer using pytorch.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-neural_transfer/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-neural_transfer/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is neural_transfer application using DEEPaaS API.&lt;/p&gt;
&lt;p&gt;The module allows you to take the content of an image and reproduce it with a new artistic style using the style of a different image.
The code is based on the &lt;a href=https://github.com/pytorch/examples/tree/master/fast_neural_style&gt;Faster Neural Style Pytorch example&lt;/a&gt; 
that implements the method described in 
"Perceptual Losses for Real-Time Style Transfer and Super-Resolution" developed by Justin Johnson, Alexandre Alahia and Li Fei-Fei [1].&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1] Justin Johnson, Alexandre Alahi, Li Fei-Fei, Perceptual Losses for Real-Time Style Transfer and Super-Resolution, 
&lt;a href=https://arxiv.org/abs/1603.08155&gt;arXiv:1603.08155 [cs.CV]&lt;/a&gt;&lt;/p&gt;</content><category term="pytorch"></category></entry><entry><title>Bird sound classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-birds-audio-classification-tf.html" rel="alternate"></link><published>2020-04-01T00:00:00+02:00</published><updated>2020-06-02T11:08:14+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2020-04-01:/modules/deep-oc-birds-audio-classification-tf.html</id><summary type="html">&lt;p&gt;Classify audio files among bird species from the Xenocanto dataset.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-birds-audio-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-birds-audio-classification-tf/master"&gt;&lt;/a&gt;
&lt;img src="https://img.shields.io/badge/-beta-blueviolet"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CAUTION: This module is in a development stage. Predictions might still not be reliable enough.&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool to perform bird sound classification with Deep Learning.
The PREDICT method expects an audio file as input (or the url of a audio file) and will return a JSON with 
the top 5 predictions. Most audio file formats are supported (see &lt;a href="https://www.ffmpeg.org/"&gt;FFMPEG&lt;/a&gt; compatible formats).&lt;/p&gt;
&lt;p&gt;We use the &lt;a href="https://www.gbif.org/dataset/b1047888-ae52-4179-9dd5-5448ea342a24"&gt;Xenocanto dataset&lt;/a&gt; [1]
which has around 350K samples covering 10K species. As an initial assessment we have trained the model on the
73 most popular species, which account for roughly 20% of all observations.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit' width=550 src='https://raw.githubusercontent.com/deephdc/DEEP-OC-birds-audio-classification-tf/master/images/workflow-birds.png'/&gt;&lt;/p&gt;
&lt;p&gt;This service is based in the &lt;a href="./deep-oc-audio-classification-tf.html"&gt;Audio Classification with Tensorflow&lt;/a&gt; model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Vellinga W (2019). &lt;a href="https://doi.org/10.15468/qv0ksn"&gt;Xeno-canto - Bird sounds from around the world&lt;/a&gt;. Xeno-canto Foundation for Nature Sounds. Occurrence dataset https://doi.org/10.15468/qv0ksn accessed via GBIF.org on 2019-10-25.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>TF Benchmarks</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-benchmarks-cnn.html" rel="alternate"></link><published>2019-12-19T00:00:00+01:00</published><updated>2020-06-02T11:08:11+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-12-19:/modules/deep-oc-benchmarks-cnn.html</id><summary type="html">&lt;p&gt;tf_cnn_benchmarks accessed via DEEPaaS API&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-benchmarks_cnn/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-benchmarks_cnn/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks"&gt;tf_cnn_benchmarks&lt;/a&gt;
from TensorFlow team accessed via &lt;a href="https://github.com/indigo-dc/DEEPaaS"&gt;DEEPaaS API&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;tf_cnn_benchmarks contains implementations of several popular convolutional models 
(e.g. Googlenet, Inception, Overfeat, Resnet, VGG), 
and is designed to be as fast as possible. tf_cnn_benchmarks supports running on a single machine 
on a single GPU and multiple GPUs
(please, note that running in distributed mode across multiple hosts is not supported by these Docker images).
See the &lt;a href="https://www.tensorflow.org/performance/performance_models"&gt;High-Performance models guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1] TF CNN Benchmarks: &lt;a href=https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks&gt;https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks&lt;/a&gt;&lt;/p&gt;</content><category term="docker"></category></entry><entry><title>Object Detection and Classification with Pytorch</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-obj-detect-pytorch.html" rel="alternate"></link><published>2019-10-17T00:00:00+02:00</published><updated>2020-06-02T11:08:13+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-10-17:/modules/deep-oc-obj-detect-pytorch.html</id><summary type="html">&lt;p&gt;A trained Region Convolutional Neural Network (Faster RCNN) for object detection and classification.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-obj_detect_pytorch/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-obj_detect_pytorch/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool for object detection and classification using deep neural networks (Faster R-CNN 
ResNet-50 FPN Architecture [1]) that were already pretrained on the &lt;a href="http://cocodataset.org/#home"&gt;COCO Dataset&lt;/a&gt;. 
The code uses the Pytorch Library, more information can be found at &lt;a href="https://pytorch.org/docs/stabletorchvision/models.html#object-detection-instance-segmentation-and-person-keypoint-detection"&gt;Pytorch-Object-Detection&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an image as input and will return a JSON with 
the predictions that are greater than the probability threshold. Let's say you have an image of a cat 
and a dog together and the probability output was 50% a dog and 80% a cat,
if you set the threshold to 70%, the only detected object will be the cat, because its probability is grater than 70%.&lt;/p&gt;
&lt;p&gt;This module works on uploaded images and gives as output the rectangle coordinates x1,y1 and x2,y2 were the
classified object is located.
It also provides you the probability of the classified detected object.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit' width='30%', src='https://raw.githubusercontent.com/SilkeDH/obj_detect_pytorch/master/reports/figures/dog_broccoli.png'/&gt;&lt;/p&gt;
&lt;p&gt;The TRAINING method uses transfer learning and expects different parameters like learning rate, name of the clases, etc.
Transfer learning focuses on storing knowledge gained while solving one problem and applying it to a different but related
problem. To achieve it, the output layer of the pre-trained model is removed and a new one with the new number of outputs is
added. Only that new layer will be trained [1]. An example of transferred learning is provided an implemented in this module.&lt;/p&gt;
&lt;p&gt;The model requires a new dataset with the classes that are going to be classified and detected. In this case the Penn-Fudan
Database for Pedestrian Detection and Segmentation was used to detect pedestrians.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit' width='70%', src='https://raw.githubusercontent.com/SilkeDH/obj_detect_pytorch/master/reports/figures/pytorchobj.png'/&gt;&lt;/p&gt;
&lt;p&gt;To try this in the module, the two dataset folders (Images and Masks) must be placed in the obj_detect_pytorch/dataset/
folder. More information about the code, the Penn Fudan Dataset and the structuring of a custom dataset can be found at &lt;a href="https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"&gt;Torchvision Object Detection Finetuning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. &lt;a href="https://arxiv.org/abs/1506.01497"&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;
arXiv preprint (2016): 1506-01497.&lt;/p&gt;
&lt;p&gt;[2]: West Jeremy, Ventura Dan, Warnick Sean. Spring Research Presentation: A Theoretical Foundation for Inductive Transfer,
Brigham Young University, College of Physical and Mathematical Sciences. (2007).&lt;/p&gt;</content><category term="services"></category></entry><entry><title>semseg_vaihingen</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-semseg-vaihingen.html" rel="alternate"></link><published>2019-09-30T00:00:00+02:00</published><updated>2020-06-02T11:08:08+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-09-30:/modules/deep-oc-semseg-vaihingen.html</id><summary type="html">&lt;p&gt;2D semantic segmentation on the Vaihingen dataset&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-semseg_vaihingen/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-semseg_vaihingen/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Example application for ISPRS 2D Semantic Labeling Contest [1]:&lt;/p&gt;
&lt;p&gt;2D semantic segmentation (Vaihingen dataset [2]) that assigns labels to multiple object categories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vaihingen dataset&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;33 patches of different sizes with 9 cm spatial resolution&lt;/li&gt;
&lt;li&gt;Manually classified into six land cover classes:&lt;/li&gt;
&lt;li&gt;Impervious surfaces, Building, Low vegetation, Tree, Clutter/background&lt;/li&gt;
&lt;li&gt;The groundtruth is provided for only 16 patches&lt;/li&gt;
&lt;li&gt;For the remaining scenes it is unreleased and used for evaluation of submitted results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;N.B.:&lt;/strong&gt; pre-trained weights can be found &lt;a href=https://nc.deep-hybrid-datacloud.eu/s/eTqJexZ5PcBxXR6&gt;here&lt;/a&gt; (unzip before use!)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html&gt;http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;[2] M. Cramer: The DGPF-Test on Digital Airborne Camera Evaluation Overview and Test Design, 
PFG Photogrammetrie, Fernerkundung, Geoinformation, vol. 2010, no. 2, pp. 73-82, 2010.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Train an audio classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-audio-classification-tf.html" rel="alternate"></link><published>2019-09-01T00:00:00+02:00</published><updated>2020-06-02T11:08:07+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-09-01:/modules/deep-oc-audio-classification-tf.html</id><summary type="html">&lt;p&gt;Train your own audio classifier with your custom dataset. It comes also pretrained on the 527 AudioSet classes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-audio-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-audio-classification-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool to perform audio classification with Deep Learning.
It allows the user to classify their samples of audio as well as training their
own classifier for a custom problem.&lt;/p&gt;
&lt;p&gt;The classifier is currently pretrained on the 527 high-level classes from the
&lt;a href="https://research.google.com/audioset/"&gt;AudioSet&lt;/a&gt; dataset.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an audio file as input (or the url of a audio file) and will return a JSON with 
the top 5 predictions. Most audio file formats are supported (see &lt;a href="https://www.ffmpeg.org/"&gt;FFMPEG&lt;/a&gt; compatible formats).&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-audio-classification-tf/master/images/demo.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, Marvin Ritter,&lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45857.pdf"&gt;'Audio set: An ontology and human-labeled dataset for audio events'&lt;/a&gt;, IEEE ICASSP, 2017.&lt;/p&gt;
&lt;p&gt;Qiuqiang Kong, Yong Xu, Wenwu Wang, Mark D. Plumbley,&lt;a href="https://arxiv.org/pdf/1711.00927.pdf"&gt;'Audio Set classification with attention model: A probabilistic perspective.'&lt;/a&gt; arXiv preprint arXiv:1711.00927 (2017).&lt;/p&gt;
&lt;p&gt;Changsong Yu, Karim Said Barsim, Qiuqiang Kong, Bin Yang ,&lt;a href="https://arxiv.org/pdf/1803.02353.pdf"&gt;'Multi-level Attention Model for Weakly Supervised Audio Classification.'&lt;/a&gt; arXiv preprint arXiv:1803.02353 (2018).&lt;/p&gt;
&lt;p&gt;S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et  al., &lt;a href="https://arxiv.org/pdf/1609.09430.pdf"&gt;'CNN architectures for large-scale audio classification,'&lt;/a&gt; arXiv preprint arXiv:1609.09430, 2016.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Body pose detection</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-posenet-tf.html" rel="alternate"></link><published>2019-07-31T00:00:00+02:00</published><updated>2020-06-02T11:08:05+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-07-31:/modules/deep-oc-posenet-tf.html</id><summary type="html">&lt;p&gt;Detect body poses in images.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-posenet-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-posenet-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool for real-time pose estimation using deep neural networks. The original model, weights, code, etc. was created by Google and can be found at https://github.com/tensorflow/tfjs-models/tree/master/posenet. &lt;/p&gt;
&lt;p&gt;PoseNet can be used to estimate either a single pose or multiple poses, meaning there is a version of the algorithm that can detect only one person in an image/video and another version that can detect multiple persons in an image/video. &lt;/p&gt;
&lt;p&gt;The PREDICT method expects an RGB image as input (or the url of an image) and returns as output the different body keypoints with the corresponding coordinates and the associated key score
&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-posenet-tf/master/images/posenet.png'/&gt;&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Speech keywords classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-speech-to-text-tf.html" rel="alternate"></link><published>2019-07-31T00:00:00+02:00</published><updated>2020-06-02T11:08:04+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-07-31:/modules/deep-oc-speech-to-text-tf.html</id><summary type="html">&lt;p&gt;Train a speech classifier to classify audio files between different keywords.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-speech-to-text-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-speech-to-text-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool to train and evaluate a speech-to-text tool using deep neural networks. The network architecture is based on one of the tutorials provided by Tensorflow &lt;a href="https://www.tensorflow.org/tutorials/sequences/audio_recognition"&gt;1&lt;/a&gt;.
The architecture used in this tutorial is based on some described in the paper Convolutional Neural Networks for Small-footprint Keyword Spotting &lt;a href="https://static.googleusercontent.com/media/research.google.com/es//pubs/archive/43969.pdf"&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are lots of different approaches to building neural network models to work with audio including recurrent networks or dilated convolutions. This model is based on the kind of convolutional network that will feel very familiar to anyone who's worked with image recognition. That may seem surprising at first though, since audio is inherently a one-dimensional continuous signal across time, not a 2D spatial problem. We define a window of time we believe our spoken words should fit into, and converting the audio signal in that window into an image. This is done by grouping the incoming audio samples into short segments, just a few milliseconds long, and calculating the strength of the frequencies across a set of bands. Each set of frequency strengths from a segment is treated as a vector of numbers, and those vectors are arranged in time order to form a two-dimensional array. This array of values can then be treated like a single-channel image and is known as a spectrogram. These spectrograms will be the input for the training.
The container does not come with any pretrained model, it has to be trained first on a dataset to be used for prediction.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an audio files as input (or the url of an audio file) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-speech-to-text-tf/master/images/speech-to-text.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>DEEP Development Environment</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-generic-dev.html" rel="alternate"></link><published>2019-03-01T00:00:00+01:00</published><updated>2020-06-02T11:08:10+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-03-01:/modules/deep-oc-generic-dev.html</id><summary type="html">&lt;p&gt;This is a Docker image for developing new modules&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-generic-dev/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-generic-dev/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a Docker image for developing new modules. It comes preinstalled with DEEPaaS, Jupyter Lab, Tensorflow, and a number of other tools (see &lt;a href="http://github.com/deephdc/DEEP-OC-generic-dev"&gt;here&lt;/a&gt;)
As you have access to the terminal of Jupyter Lab, you can develop and debug your application, add your preferred tools.&lt;/p&gt;</content><category term="development"></category></entry><entry><title>DEEP OC Massive Online Data Streams</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-mods.html" rel="alternate"></link><published>2019-02-19T00:00:00+01:00</published><updated>2020-06-02T11:07:38+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-02-19:/modules/deep-oc-mods.html</id><summary type="html">&lt;p&gt;Deep learning for proactive network monitoring and security protection.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-mods/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-mods/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The use case challenges proactive network monitoring for security protection of computing infrastructures [1].
It builds an intelligent module as a machine learning application leveraging deep learning modeling 
to enhance functionality of intrusion detection system supervising network traffic flows.
Preserving historical data, cyber security for such centers can be enhanced in hybrid way [3,4,5] 
using machine learning techniques, especially when large IT infrastructures and devices products 
a huge amount of dataflows continuously and dynamically.&lt;/p&gt;
&lt;p&gt;The principle of this deep learning module (as a part of the use case) is proactive time-series forecasting. 
It builds prediction models capable to produce a system behaviour estimation near future. 
The discrepancy between the prediction and the reality gives an indication of anomaly (i.e. anomaly detection).&lt;/p&gt;
&lt;p&gt;The challenge of the solution is it aims to scalable edge technologies [4] to support
extensive data analysis and modelling as well as to improve the cyber-resilience in hybrid combination
in real-time with the building intelligence module using neural networks and deep learning.&lt;/p&gt;
&lt;p&gt;Deep learning architectures [2] available in this module are: MLP, CNN, autoencoder MLP, 
LSTM, GRU, bidirectional LSTM, sequence to sequence LSTM, stacked LSTM, attention LSTM, TCN, and stackedTCN.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Nguyen G., Dlugolinsky S., Tran V., Lopez Garcia A.: Deep learning for proactive network monitoring and security protection. IEEE Access, 2020, Volume 8, ISSN 2169-3536, DOI 10.1109/ACCESS.2020.2968718.&lt;/p&gt;
&lt;p&gt;[2]: Nguyen G., Dlugolinsky S., Bobak M., Tran V., Lopez Garcia A., Heredia I., Malik P., Hluchy L.: Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey. Artificial Intelligence Review, Volume 52, Issue 1, pp. 77-124, ISSN 0269-2821, DOI 10.1007/s10462-018-09679-z. Springer Nature, 2019.&lt;/p&gt;
&lt;p&gt;[3]: Nguyen G., Nguyen, M., Tran, D. and Hluchy L.: A heuristics approach to mine behavioural data logs in mobile malware detection system. Data &amp;amp; Knowledge Engineering, Volume 115, pp. 129-151, ISSN 0169-023X, DOI 10.1016/j.datak.2018.03.002. Elsevier, 2018.&lt;/p&gt;
&lt;p&gt;[4]: Bhattacharyya, D.K. and Kalita, J.K., 2013. Network anomaly detection: A machine learning perspective. Chapman and Hall/CRC.&lt;/p&gt;
&lt;p&gt;[5]: Dua, S. and Du, X., 2016. Data mining and machine learning in cybersecurity. Auerbach Publications.&lt;/p&gt;</content><category term="services"></category></entry><entry><title>Conus species classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-conus-classification-tf.html" rel="alternate"></link><published>2019-01-01T00:00:00+01:00</published><updated>2020-06-02T11:07:50+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-01-01:/modules/deep-oc-conus-classification-tf.html</id><summary type="html">&lt;p&gt;Classify conus images among 70 species.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-conus-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-conus-classification-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Citizen science has become a powerful force for scientific inquiry, providing researchers with access to a vast array
of data points while connecting non scientists to the real process of science. This citizen-researcher relationship
creates a very interesting synergy, allowing for the creation, execution, and analysis of research projects.
With this in mind, a Convolutional Neural Network has been trained to identify conus marine snails at species
level [1] in collaboration with the &lt;a href="http://www.mncn.csic.es/"&gt;Natural Science Museum of Madrid&lt;/a&gt;. The taxonomy
of these snails has changed significantly several times during recent years and the introduction of Deep Learning
techniques allowing to classify them is a very valuable tool for the experts.&lt;/p&gt;
&lt;p&gt;This Docker container contains a trained Convolutional Neural network optimized for conus identification using
images. The architecture used is an Xception [2] network using Keras on top of Tensorflow.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an RGB image as input (or the url of an RGB image) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;The training dataset has been provided by the &lt;a href="http://www.mncn.csic.es/"&gt;Natural Science Museum of Madrid&lt;/a&gt; and
it consists on a dataset containing images of 68 species of conus covering three different regions: the Panamic
region; the South African region; and the Western Atlantic and Mediterranean region.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-conus-classification-tf/master/images/conus.png'/&gt;&lt;/p&gt;
&lt;p&gt;This service is based in the &lt;a href="./deep-oc-image-classification-tensorflow.html"&gt;Image Classification with Tensorflow&lt;/a&gt; model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Puillandre, N.; Duda, T.F.; Meyer, C.; Olivera, B.M.; Bouchet, P. (2014). &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4541476/"&gt;One, four or 100 genera? A new classification of the cone snails&lt;/a&gt;. Journal of Molluscan Studies. 81 (1): 1-23.&lt;/p&gt;
&lt;p&gt;[2]: Chollet, Francois. &lt;a href="https://arxiv.org/abs/1610.02357"&gt;Xception: Deep learning with depthwise separable convolutions&lt;/a&gt; arXiv preprint (2017): 1610-02357.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Train an image classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-image-classification-tf.html" rel="alternate"></link><published>2019-01-01T00:00:00+01:00</published><updated>2020-06-02T11:07:44+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-01-01:/modules/deep-oc-image-classification-tf.html</id><summary type="html">&lt;p&gt;Train your own image classifier with your custom dataset. It comes also pretrained on the 1K ImageNet classes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-image-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-image-classification-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The deep learning revolution has brought significant advances in a number of fields [1], primarily linked to
image and speech recognition. The standardization of image classification tasks like the &lt;a href="http://www.image-net.org/challenges/LSVRC/"&gt;ImageNet Large Scale
Visual Recognition Challenge&lt;/a&gt; [2] has resulted in a reliable way to
compare top performing architectures.&lt;/p&gt;
&lt;p&gt;This Docker container contains the tools to train an image classifier on your personal dataset. It is a highly
customizable tool  that let's you choose between tens of different &lt;a href="https://github.com/keras-team/keras-applications"&gt;top performing architectures&lt;/a&gt;
and training parameters.&lt;/p&gt;
&lt;p&gt;The container also comes with a pretrained general-purpose image classifier trained on ImageNet.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an RGB image as input (or the url of an RGB image) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-image-classification-tf/master/images/imagenet.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Yann LeCun, Yoshua Bengio, and Geofrey Hinton. &lt;a href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf"&gt;Deep learning&lt;/a&gt;. Nature, 521(7553):436-444, May 2015.&lt;/p&gt;
&lt;p&gt;[2]: Olga Russakovsky et al. &lt;a href="https://arxiv.org/abs/1409.0575"&gt;ImageNet Large Scale Visual Recognition Challenge&lt;/a&gt;. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Chest x-ray image classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-image-classification-tf-dicom.html" rel="alternate"></link><published>2019-01-01T00:00:00+01:00</published><updated>2020-06-02T11:08:18+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-01-01:/modules/deep-oc-image-classification-tf-dicom.html</id><summary type="html">&lt;p&gt;Classify chest x-ray images in patological and non patological with this x-ray classifier.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-image-classification-tf-dicom/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-image-classification-tf-dicom/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The deep learning revolution has brought significant advances in a number of fields [1], primarily linked to
image and speech recognition. The standardization of image classification tasks like the &lt;a href="http://www.image-net.org/challenges/LSVRC/"&gt;ImageNet Large Scale
Visual Recognition Challenge&lt;/a&gt; [2] has resulted in a reliable way to
compare top performing architectures.&lt;/p&gt;
&lt;p&gt;This Docker container contains the tools to train an image classifier on your personal dataset. It is a highly
customizable tool  that let's you choose between tens of different &lt;a href="https://github.com/keras-team/keras-applications"&gt;top performing architectures&lt;/a&gt;
and training parameters.&lt;/p&gt;
&lt;p&gt;The container also comes with a pretrained general-purpose image classifier trained on ImageNet.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an RGB image as input (or the url of an RGB image) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-image-classification-tf-dicom/master/images/imagenet.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Yann LeCun, Yoshua Bengio, and Geofrey Hinton. &lt;a href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf"&gt;Deep learning&lt;/a&gt;. Nature, 521(7553):436-444, May 2015.&lt;/p&gt;
&lt;p&gt;[2]: Olga Russakovsky et al. &lt;a href="https://arxiv.org/abs/1409.0575"&gt;ImageNet Large Scale Visual Recognition Challenge&lt;/a&gt;. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Phytoplankton species classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-phytoplankton-classification-tf.html" rel="alternate"></link><published>2019-01-01T00:00:00+01:00</published><updated>2020-06-02T11:07:52+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-01-01:/modules/deep-oc-phytoplankton-classification-tf.html</id><summary type="html">&lt;p&gt;Classify phytoplankton images among 60 classes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-phytoplankton-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-phytoplankton-classification-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Citizen science has become a powerful force for scientific inquiry, providing researchers with access to a vast array of
data points while connecting non scientists to the real process of science. This citizen-researcher relationship
creates a very interesting synergy, allowing for the creation, execution, and analysis of research projects. 
With this in mind, a Convolutional Neural Network has been trained to identify phytoplankton in collaboration
with the &lt;a href="http://www.vliz.be/"&gt;Vlaams Instituut voor de Zee&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This Docker container contains a trained Convolutional Neural network optimized for phytoplankton identification using
images. The architecture used is an Xception [1] network using Keras on top of Tensorflow.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an RGB image as input (or the url of an RGB image) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;As training dataset we have used a collection of images from the &lt;a href="http://www.vliz.be/"&gt;Vlaams Instituut voor de Zee&lt;/a&gt;
which consists of around 650K images from 60 classes of phytoplankton.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-phytoplankton-classification-tf/master/images/phytoplankton.png'/&gt;&lt;/p&gt;
&lt;p&gt;This service is based in the &lt;a href="./deep-oc-image-classification-tensorflow.html"&gt;Image Classification with Tensorflow&lt;/a&gt; model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Chollet, Francois. &lt;a href="https://arxiv.org/abs/1610.02357"&gt;Xception: Deep learning with depthwise separable convolutions&lt;/a&gt; arXiv preprint (2017): 1610-02357.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Plants species classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-plants-classification-tf.html" rel="alternate"></link><published>2019-01-01T00:00:00+01:00</published><updated>2020-06-02T11:07:49+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-01-01:/modules/deep-oc-plants-classification-tf.html</id><summary type="html">&lt;p&gt;Classify plant images among 10K species from the iNaturalist dataset.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-plants-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-plants-classification-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The deep learning revolution has brought significant advances in a number of fields [1], primarily linked to
image and speech recognition. The standardization of image classification tasks like the &lt;a href="http://www.image-net.org/challenges/LSVRC/"&gt;ImageNet Large Scale
Visual Recognition Challenge&lt;/a&gt; [2] has resulted in a reliable way to
compare top performing architectures.&lt;/p&gt;
&lt;p&gt;The use of deep learning for plant classification is not novel [3, 4] but has mainly focused in leaves and has
been restricted to a limited amount of species, therefore making it of limited use for large-scale biodiversity
monitoring purposes.&lt;/p&gt;
&lt;p&gt;This Docker container contains a trained Convolutional Neural network optimized for plant identification using
images. The architecture used is an Xception [5] network using Keras on top of Tensorflow. A detailed article
about this network and the results obtained with it can be found in [6].&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an RGB image as input (or the url of an RGB image) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;The original training dataset was the great collection of images which are available in &lt;a href="https://identify.plantnet.org/"&gt;PlantNet&lt;/a&gt; under a 
Creative-Common AttributionShareAlike 2.0 license. It consists of around 250K images belonging to more than 6K
plant species of Western Europe. These species are distributed in 1500 genera and 200 families.&lt;/p&gt;
&lt;p&gt;A new iteration of the application has been trained using plant images from &lt;a href="https://www.inaturalist.org/"&gt;iNaturalist&lt;/a&gt;.
This dataset has around 4.4M observations with 7M images from 58K worldwide species.
We have restricted our training to the 10K most popular species.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-plants-classification-tf/master/images/plants.png'/&gt;&lt;/p&gt;
&lt;p&gt;This service is based in the &lt;a href="./deep-oc-image-classification-tensorflow.html"&gt;Image Classification with Tensorflow&lt;/a&gt; model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Yann LeCun, Yoshua Bengio, and Geofrey Hinton. &lt;a href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf"&gt;Deep learning&lt;/a&gt;. Nature, 521(7553):436-444, May 2015.&lt;/p&gt;
&lt;p&gt;[2]: Olga Russakovsky et al. &lt;a href="https://arxiv.org/abs/1409.0575"&gt;ImageNet Large Scale Visual Recognition Challenge&lt;/a&gt;. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.&lt;/p&gt;
&lt;p&gt;[3]: Sue Han Lee, Chee Seng Chan, Paul Wilkin, and Paolo Remagnino. &lt;a href="https://arxiv.org/abs/1506.08425"&gt;Deep-plant: Plant identification with convolutional neural networks&lt;/a&gt;, 2015.&lt;/p&gt;
&lt;p&gt;[4]: Mads Dyrmann, Henrik Karstoft, and Henrik Skov Midtiby. &lt;a href="https://www.sciencedirect.com/science/article/pii/S1537511016301465"&gt;Plant species classification using deep convolutional neural network.&lt;/a&gt; Biosystems Engineering, 151:72-80, 2016.&lt;/p&gt;
&lt;p&gt;[5]: Chollet, Francois. &lt;a href="https://arxiv.org/abs/1610.02357"&gt;Xception: Deep learning with depthwise separable convolutions&lt;/a&gt; arXiv preprint (2017): 1610-02357.&lt;/p&gt;
&lt;p&gt;[6]: Heredia, Ignacio. &lt;a href="https://arxiv.org/abs/1706.03736"&gt;Large-scale plant classification with deep neural networks.&lt;/a&gt; Proceedings of the Computing Frontiers Conference. ACM, 2017.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Upscale multispectral satellites images</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-satsr.html" rel="alternate"></link><published>2019-01-01T00:00:00+01:00</published><updated>2020-06-02T11:08:02+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-01-01:/modules/deep-oc-satsr.html</id><summary type="html">&lt;p&gt;Upscale (superresolve) low resolution bands to high resolution in multispectral satellite imagery.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-satsr/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-satsr/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With the latest missions launched by the European Space Agency (ESA) and National Aeronautics and Space Administration (NASA)
equipped with the latest technologies in multi-spectral sensors, we face an unprecedented amount of data with spatial and
temporal resolutions never reached before. Exploring the potential of this data with state-of-the-art AI techniques like
Deep Learning, could potentially change the way we think about and protect our planet's resources.&lt;/p&gt;
&lt;p&gt;This Docker container contains  a plug-and-play tool to perform super-resolution on satellite imagery.
It uses Deep Learning to provide a better performing alternative to classical pansharpening (more details in [1]).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Minimum requirements&lt;/strong&gt; Working with satellite imagery is a memory intensive task, so an absolute minimum is
16 GB of RAM memory. But if you want to work with full images (not small patches) you will probably need in the
order of 50 GB. If memory requirements are not met, weird Tensorflow shape errors can appear.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects a compressed file (zip or tar) containing a complete tile of the satellite. These
tiles are different for each satellite type and can be downloaded in the respective official satellite's
repositories. We provide nevertheless &lt;a href="https://github.com/deephdc/satsr/blob/master/reports/additional_notes.md"&gt;some samples&lt;/a&gt;
for each satellite so that the user can test the module.
The output is a GeoTiff file with the super-resolved region.&lt;/p&gt;
&lt;p&gt;Right now we are supporting super-resolution for the following satellites:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://sentinel.esa.int/web/sentinel/missions/sentinel-2"&gt;Sentinel 2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://landsat.gsfc.nasa.gov/landsat-8/"&gt;Landsat 8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ncc.nesdis.noaa.gov/VIIRS/"&gt;VIIRS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://terra.nasa.gov/about/terra-instruments/modis"&gt;MODIS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More information on the satellites and processing levels that are supported can be found &lt;a href="https://github.com/deephdc/satsr/blob/master/reports/additional_notes.md"&gt;here&lt;/a&gt;
along with some &lt;a href="https://github.com/deephdc/satsr/tree/master/reports/figures"&gt;demo images&lt;/a&gt; of the super-resolutions performed in non-training data.&lt;/p&gt;
&lt;p&gt;If you want to perform super-resolution on another satellite, go to the &lt;a href="https://github.com/deephdc/satsr#train-other-satellites"&gt;training section&lt;/a&gt;
to see how you can easily add support for additional satellites. We are happy to accept PRs!&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-satsr/master/images/satsr.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Lanaras, C., Bioucas-Dias, J., Galliani, S., Baltsavias, E., &amp;amp; Schindler, K. (2018). &lt;a href="https://arxiv.org/abs/1803.04271"&gt;Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network&lt;/a&gt;. ISPRS Journal of Photogrammetry and Remote Sensing, 146, 305-319.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Seed species classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-seeds-classification-tf.html" rel="alternate"></link><published>2019-01-01T00:00:00+01:00</published><updated>2020-06-02T11:07:59+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-01-01:/modules/deep-oc-seeds-classification-tf.html</id><summary type="html">&lt;p&gt;Classify seeds images among 700K species.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-seeds-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-seeds-classification-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Citizen science has become a powerful force for scientific inquiry, providing researchers with access to a vast array of
data points while connecting non scientists to the real process of science. This citizen-researcher relationship
creates a very interesting synergy, allowing for the creation, execution, and analysis of research projects. 
With this in mind, a Convolutional Neural Network has been trained to identify seed images in collaboration 
with &lt;a href="http://www.rjb.csic.es"&gt;Spanish Royal Botanical Garden&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This Docker container contains a trained Convolutional Neural network optimized for seeds identification using
images. The architecture used is an Xception [1] network using Keras on top of Tensorflow.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an RGB image as input (or the url of an RGB image) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;As training dataset we have used a collection of images from the &lt;a href="http://www.rjb.csic.es"&gt;Spanish Royal Botanical Garden&lt;/a&gt;
which consists of around 28K images from 743 species and 493 genera.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-seeds-classification-tf/master/images/seeds.png'/&gt;&lt;/p&gt;
&lt;p&gt;This service is based in the &lt;a href="./deep-oc-image-classification-tensorflow.html"&gt;Image Classification with Tensorflow&lt;/a&gt; model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Chollet, Francois. &lt;a href="https://arxiv.org/abs/1610.02357"&gt;Xception: Deep learning with depthwise separable convolutions&lt;/a&gt; arXiv preprint (2017): 1610-02357.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Dogs breed detector</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-dogs-breed-det.html" rel="alternate"></link><published>2018-11-18T00:00:00+01:00</published><updated>2020-06-02T11:07:34+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2018-11-18:/modules/deep-oc-dogs-breed-det.html</id><summary type="html">&lt;p&gt;Identify a dogs breed on the image (133 known breeds)&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-dogs_breed_det/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-dogs_breed_det/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The application applies Transfer learning for dog's breed identification, which is implemented by the means of Tensorflow and Keras:&lt;/p&gt;
&lt;p&gt;From a pre-trained CNN model (VGG16 | VGG19 | Resnet50 | InceptionV3 [1]) the last layer is removed,
then new Fully Connected (FC) layers are added, which are trained on the dog's dataset.&lt;/p&gt;
&lt;p&gt;The original dataset ([2]) consists of 8351 dog's images for 133 breeds divided into: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;training set (6680 pictures)&lt;/li&gt;
&lt;li&gt;validation set (835)&lt;/li&gt;
&lt;li&gt;test set (836)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and amounts for &lt;strong&gt;1080 MB&lt;/strong&gt; in zipped format (see the dataset link).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;N.B.:&lt;/strong&gt; pre-trained weights can be found &lt;a href=https://nc.deep-hybrid-datacloud.eu/s/D7DLWcDsRoQmRMN&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1] CNN articles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VGG: Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition.
 CoRR abs/1409.1556 (2014); http://arxiv.org/abs/1409.1556&lt;/li&gt;
&lt;li&gt;Resnet: He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun: Deep residual learning for image recognition. 
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016. https://arxiv.org/abs/1512.03385&lt;/li&gt;
&lt;li&gt;InceptionV3: Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna; The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016, pp. 2818-2826. https://arxiv.org/abs/1512.00567&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[2] Dogs dataset: &lt;a href=https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip&gt;
https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip&lt;/a&gt;&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>DEEP OC Retinopathy Test</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-retinopathy-test.html" rel="alternate"></link><published>2018-10-02T00:00:00+02:00</published><updated>2020-06-02T11:07:41+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2018-10-02:/modules/deep-oc-retinopathy-test.html</id><summary type="html">&lt;p&gt;A Tensorflow model to classify Retinopathy.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-retinopathy_test/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-retinopathy_test/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This use case is concerned with the classification of biomedical images (of the retina) into five disease categories or stages (from healthy to severe) using a deep learning approach.  Retinopathy is a fast-growing cause of blindness worldwide, over 400 million people at risk from diabetic retinopathy alone. The disease can be successfully treated if it is detected early. Colour fundus retinal photography uses a fundus camera (a specialized low power microscope with an attached camera) to record color images of the condition of the interior surface of the eye, in order to document the presence of disorders and monitor their change over time. Specialized medical experts interpret such images and are able to detect the presence and stage of retinal eye disease such as diabetic retinopathy. However, due to a lack of suitably qualified medical specialists in many parts of the world a comprehensive detection and treatment of the disease is difficult. This use case focuses on a deep learning approach to automated classification of retinopathy based on color fundus retinal photography images.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>DEEP Data Science template</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-data-science-template.html" rel="alternate"></link><published>2018-08-01T00:00:00+02:00</published><updated>2019-02-16T00:00:00+01:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2018-08-01:/modules/deep-data-science-template.html</id><summary type="html">&lt;p&gt;A logical, reasonably standardized, but flexible project structure for doing and sharing data science work. Based on more general &lt;a href="http://drivendata.github.io/cookiecutter-data-science/"&gt;data science template&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;A logical, reasonably standardized, but flexible project structure for doing and sharing data science work.
Based on more general &lt;a href="http://drivendata.github.io/cookiecutter-data-science/"&gt;template&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To simplify the development and in an easy way integrate a model with &lt;a href="https://github.com/indigo-dc/DEEPaaS"&gt;DEEPaaS API&lt;/a&gt;,
a project template, &lt;a href="https://github.com/indigo-dc/cookiecutter-data-science"&gt;DEEP Data Science template&lt;/a&gt; is offered.&lt;/p&gt;
&lt;h3&gt;Requirements to use the cookiecutter template:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.5&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cookiecutter.readthedocs.org/en/latest/installation.html"&gt;Cookiecutter Python package&lt;/a&gt; &amp;gt;= 1.4.0: This can be installed with pip by or conda depending on how you manage your Python packages:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ pip install cookiecutter
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ conda config --add channels conda-forge
$ conda install cookiecutter
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;To start a new project, run:&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;cookiecutter https://github.com/indigo-dc/cookiecutter-data-science&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;The resulting directories&lt;/h3&gt;
&lt;p&gt;Once you answer all the questions, two directories will be created:
 - DEEP-OC-&lt;your_project&gt;
 - &lt;your_project&gt;&lt;/p&gt;
&lt;p&gt;each directory is a git repository and has two branches: &lt;code&gt;master&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;The directory structure of &lt;your_project&gt; looks like this:&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;LICENSE&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;README&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;              &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt; &lt;span class="n"&gt;README&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;developers&lt;/span&gt; &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;this&lt;/span&gt; &lt;span class="n"&gt;project&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;                &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;immutable&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;docs&lt;/span&gt;                   &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="n"&gt;Sphinx&lt;/span&gt; &lt;span class="n"&gt;project&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;see&lt;/span&gt; &lt;span class="n"&gt;sphinx&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;details&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;                 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Trained&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;serialized&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;or&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="n"&gt;summaries&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;notebooks&lt;/span&gt;              &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Jupyter&lt;/span&gt; &lt;span class="n"&gt;notebooks&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Naming&lt;/span&gt; &lt;span class="n"&gt;convention&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="nf"&gt;number&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ordering&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;                             &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;creator&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="nf"&gt;initials&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;many&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="n"&gt;development&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;                             &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;short&lt;/span&gt; &lt;span class="ss"&gt;`_`&lt;/span&gt; &lt;span class="n"&gt;delimited&lt;/span&gt; &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;                             &lt;span class="ss"&gt;`1.0-jqp-initial_data_exploration.ipynb`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="k"&gt;references&lt;/span&gt;             &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Data&lt;/span&gt; &lt;span class="n"&gt;dictionaries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;manuals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="k"&gt;all&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt; &lt;span class="n"&gt;explanatory&lt;/span&gt; &lt;span class="n"&gt;materials&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;reports&lt;/span&gt;                &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Generated&lt;/span&gt; &lt;span class="n"&gt;analysis&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;HTML&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PDF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LaTeX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;etc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;figures&lt;/span&gt;            &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Generated&lt;/span&gt; &lt;span class="n"&gt;graphics&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;figures&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;be&lt;/span&gt; &lt;span class="n"&gt;used&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;reporting&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;requirements&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;       &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;requirements&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;reproducing&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;analysis&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;                             &lt;span class="n"&gt;generated&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="ss"&gt;`pip freeze &amp;gt; requirements.txt`&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;requirements&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;  &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;requirements&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;setup&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;               &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;makes&lt;/span&gt; &lt;span class="n"&gt;project&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="nf"&gt;installable&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="p"&gt;.)&lt;/span&gt; &lt;span class="n"&gt;so&lt;/span&gt; &lt;span class="err"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;cookiecutter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repo_name&lt;/span&gt;&lt;span class="err"&gt;}}&lt;/span&gt; &lt;span class="n"&gt;can&lt;/span&gt; &lt;span class="n"&gt;be&lt;/span&gt; &lt;span class="n"&gt;imported&lt;/span&gt;
&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="err"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;cookiecutter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repo_name&lt;/span&gt;&lt;span class="err"&gt;}}&lt;/span&gt;    &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Source&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="k"&gt;use&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;this&lt;/span&gt; &lt;span class="n"&gt;project&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;        &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Makes&lt;/span&gt; &lt;span class="err"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;cookiecutter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repo_name&lt;/span&gt;&lt;span class="err"&gt;}}&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;Python&lt;/span&gt; &lt;span class="n"&gt;module&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;            &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Scripts&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;download&lt;/span&gt; &lt;span class="k"&gt;or&lt;/span&gt; &lt;span class="n"&gt;generate&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;make_dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;           &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Scripts&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;turn&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;modeling&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;build_features&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;             &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Scripts&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;make&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;deep_api&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;    &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Main&lt;/span&gt; &lt;span class="n"&gt;script&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;integration&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;DEEP&lt;/span&gt; &lt;span class="n"&gt;API&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;tests&lt;/span&gt;              &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Scripts&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;perfrom&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="n"&gt;testing&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;   &lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;visualization&lt;/span&gt;      &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;Scripts&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="k"&gt;create&lt;/span&gt; &lt;span class="n"&gt;exploratory&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="n"&gt;oriented&lt;/span&gt; &lt;span class="n"&gt;visualizations&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;       &lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;visualize&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;tox&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ini&lt;/span&gt;                &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;tox&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;settings&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;running&lt;/span&gt; &lt;span class="n"&gt;tox&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;see&lt;/span&gt; &lt;span class="n"&gt;tox&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;testrun&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;The directory structure of DEEP-OC-&lt;your_project&gt; looks like this:&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;├─ Dockerfile             Describes main steps on integrationg DEEPaaS API and&lt;/span&gt;
&lt;span class="err"&gt;│                         &amp;lt;your_project&amp;gt; application in one Docker image&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├─ Jenkinsfile            Describes basic Jenkins CI/CD pipeline&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├─ LICENSE                License file&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├─ README.md              README for developers and users.&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├─ docker-compose.yml     Allows running the application with various configurations via docker-compose&lt;/span&gt;
&lt;span class="err"&gt;│&lt;/span&gt;
&lt;span class="err"&gt;├─ metadata.json          Defines information propagated to the [DEEP Open Catalog](https://marketplace.deep-hybrid-datacloud.eu)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Documentation&lt;/h3&gt;
&lt;p&gt;More extended documentation can be found &lt;a href="http://docs.deep-hybrid-datacloud.eu/en/latest/user/overview/cookiecutter-template.html"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="tools"></category></entry></feed>