<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>DEEP Open Catalog - api-v1</title><link href="https://marketplace.deep-hybrid-datacloud.eu/" rel="alternate"></link><link href="https://marketplace.deep-hybrid-datacloud.eu/feeds/api-v1.atom.xml" rel="self"></link><id>https://marketplace.deep-hybrid-datacloud.eu/</id><updated>2020-02-04T00:38:16+01:00</updated><entry><title>TF Benchmarks</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/tf-benchmarks.html" rel="alternate"></link><published>2019-12-19T00:00:00+01:00</published><updated>2020-02-04T00:38:16+01:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-12-19:/modules/tf-benchmarks.html</id><summary type="html">&lt;p&gt;tf_cnn_benchmarks accessed via DEEPaaS API&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-benchmarks_api/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-benchmarks_api/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks"&gt;tf_cnn_benchmarks&lt;/a&gt;
from TensorFlow team accessed via DEEPaaS API&lt;/p&gt;
&lt;p&gt;tf_cnn_benchmarks contains implementations of several popular convolutional models 
(e.g. Googlenet, Inception, Overfeat, Resnet, VGG), 
and is designed to be as fast as possible. tf_cnn_benchmarks supports both running on a single machine 
or running in distributed mode across multiple hosts.
See the &lt;a href="https://www.tensorflow.org/performance/performance_models"&gt;High-Performance models guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1] TF CNN Benchmarks: &lt;a href=https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks&gt;https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks&lt;/a&gt;&lt;/p&gt;</content><category term="docker"></category></entry><entry><title>semseg_vaihingen</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/semseg_vaihingen.html" rel="alternate"></link><published>2019-09-30T00:00:00+02:00</published><updated>2020-02-04T00:38:14+01:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-09-30:/modules/semseg_vaihingen.html</id><summary type="html">&lt;p&gt;2D semantic segmentation on the Vaihingen dataset&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-semseg_vaihingen/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-semseg_vaihingen/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Example application for ISPRS 2D Semantic Labeling Contest [1]:&lt;/p&gt;
&lt;p&gt;2D semantic segmentation (Vaihingen dataset [2]) that assigns labels to multiple object categories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vaihingen dataset&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;33 patches of different sizes with 9 cm spatial resolution&lt;/li&gt;
&lt;li&gt;Manually classified into six land cover classes:&lt;/li&gt;
&lt;li&gt;Impervious surfaces, Building, Low vegetation, Tree, Clutter/background&lt;/li&gt;
&lt;li&gt;The groundtruth is provided for only 16 patches&lt;/li&gt;
&lt;li&gt;For the remaining scenes it is unreleased and used for evaluation of submitted results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;N.B.:&lt;/strong&gt; pre-trained weights can be found &lt;a href=https://nc.deep-hybrid-datacloud.eu/s/eTqJexZ5PcBxXR6&gt;here&lt;/a&gt; (unzip before use!)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html&gt;http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;[2] M. Cramer: The DGPF-Test on Digital Airborne Camera Evaluation Overview and Test Design, 
PFG Photogrammetrie, Fernerkundung, Geoinformation, vol. 2010, no. 2, pp. 73-82, 2010.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Speech keywords classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/speech-keywords-classifier.html" rel="alternate"></link><published>2019-07-31T00:00:00+02:00</published><updated>2020-02-04T00:38:08+01:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-07-31:/modules/speech-keywords-classifier.html</id><summary type="html">&lt;p&gt;Train a speech classifier to classify audio files between different keywords.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-speech-to-text-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-speech-to-text-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool to train and evaluate a speech-to-text tool using deep neural networks. The network architecture is based on one of the tutorials provided by Tensorflow &lt;a href="https://www.tensorflow.org/tutorials/sequences/audio_recognition"&gt;1&lt;/a&gt;.
The architecture used in this tutorial is based on some described in the paper Convolutional Neural Networks for Small-footprint Keyword Spotting &lt;a href="https://static.googleusercontent.com/media/research.google.com/es//pubs/archive/43969.pdf"&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are lots of different approaches to building neural network models to work with audio including recurrent networks or dilated convolutions. This model is based on the kind of convolutional network that will feel very familiar to anyone who's worked with image recognition. That may seem surprising at first though, since audio is inherently a one-dimensional continuous signal across time, not a 2D spatial problem. We define a window of time we believe our spoken words should fit into, and converting the audio signal in that window into an image. This is done by grouping the incoming audio samples into short segments, just a few milliseconds long, and calculating the strength of the frequencies across a set of bands. Each set of frequency strengths from a segment is treated as a vector of numbers, and those vectors are arranged in time order to form a two-dimensional array. This array of values can then be treated like a single-channel image and is known as a spectrogram. These spectrograms will be the input for the training.
The container does not come with any pretrained model, it has to be trained first on a dataset to be used for prediction.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an audio files as input (or the url of an audio file) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-speech-to-text-tf/master/images/speech-to-text.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Upscale multispectral satellites images</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/upscale-multispectral-satellites-images.html" rel="alternate"></link><published>2019-01-01T00:00:00+01:00</published><updated>2020-02-04T00:38:06+01:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-01-01:/modules/upscale-multispectral-satellites-images.html</id><summary type="html">&lt;p&gt;Upscale (superresolve) low resolution bands to high resolution in multispectral satellite imagery.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-satsr/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-satsr/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With the latest missions launched by the European Space Agency (ESA) and National Aeronautics and Space Administration (NASA)
equipped with the latest technologies in multi-spectral sensors, we face an unprecedented amount of data with spatial and
temporal resolutions never reached before. Exploring the potential of this data with state-of-the-art AI techniques like
Deep Learning, could potentially change the way we think about and protect our planet's resources.&lt;/p&gt;
&lt;p&gt;This Docker container contains  a plug-and-play tool to perform super-resolution on satellite imagery.
It uses Deep Learning to provide a better performing alternative to classical pansharpening (more details in [1]).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Minimum requirements&lt;/strong&gt; Working with satellite imagery is a memory intensive task, so an absolute minimum is
16 GB of RAM memory. But if you want to work with full images (not small patches) you will probably need in the
order of 50 GB. If memory requirements are not met, weird Tensorflow shape errors can appear.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects a compressed file (zip or tar) containing a complete tile of the satellite. These
tiles are different for each satellite type and can be downloaded in the respective official satellite's
repositories. We provide nevertheless &lt;a href="https://github.com/deephdc/satsr/blob/master/reports/additional_notes.md"&gt;some samples&lt;/a&gt;
for each satellite so that the user can test the module.
The output is a GeoTiff file with the super-resolved region.&lt;/p&gt;
&lt;p&gt;Right now we are supporting super-resolution for the following satellites:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://sentinel.esa.int/web/sentinel/missions/sentinel-2"&gt;Sentinel 2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://landsat.gsfc.nasa.gov/landsat-8/"&gt;Landsat 8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ncc.nesdis.noaa.gov/VIIRS/"&gt;VIIRS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://terra.nasa.gov/about/terra-instruments/modis"&gt;MODIS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More information on the satellites and processing levels that are supported can be found &lt;a href="https://github.com/deephdc/satsr/blob/master/reports/additional_notes.md"&gt;here&lt;/a&gt;
along with some &lt;a href="https://github.com/deephdc/satsr/tree/master/reports/figures"&gt;demo images&lt;/a&gt; of the super-resolutions performed in non-training data.&lt;/p&gt;
&lt;p&gt;If you want to perform super-resolution on another satellite, go to the &lt;a href="https://github.com/deephdc/satsr#train-other-satellites"&gt;training section&lt;/a&gt;
to see how you can easily add support for additional satellites. We are happy to accept PRs!&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-satsr/master/images/satsr.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Lanaras, C., Bioucas-Dias, J., Galliani, S., Baltsavias, E., &amp;amp; Schindler, K. (2018). &lt;a href="https://arxiv.org/abs/1803.04271"&gt;Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network&lt;/a&gt;. ISPRS Journal of Photogrammetry and Remote Sensing, 146, 305-319.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>DEEP OC Retinopathy Test</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-retinopathy-test.html" rel="alternate"></link><published>2018-10-02T00:00:00+02:00</published><updated>2020-02-04T00:37:54+01:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2018-10-02:/modules/deep-oc-retinopathy-test.html</id><summary type="html">&lt;p&gt;A Tensorflow model to classify Retinopathy.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-retinopathy_test/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-retinopathy_test/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This use case is concerned with the classification of biomedical images (of the retina) into five disease categories or stages (from healthy to severe) using a deep learning approach.  Retinopathy is a fast-growing cause of blindness worldwide, over 400 million people at risk from diabetic retinopathy alone. The disease can be successfully treated if it is detected early. Colour fundus retinal photography uses a fundus camera (a specialized low power microscope with an attached camera) to record color images of the condition of the interior surface of the eye, in order to document the presence of disorders and monitor their change over time. Specialized medical experts interpret such images and are able to detect the presence and stage of retinal eye disease such as diabetic retinopathy. However, due to a lack of suitably qualified medical specialists in many parts of the world a comprehensive detection and treatment of the disease is difficult. This use case focuses on a deep learning approach to automated classification of retinopathy based on color fundus retinal photography images.&lt;/p&gt;</content><category term="tensorflow"></category></entry></feed>