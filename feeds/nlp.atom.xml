<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>DEEP Open Catalog - nlp</title><link href="https://marketplace.deep-hybrid-datacloud.eu/" rel="alternate"></link><link href="https://marketplace.deep-hybrid-datacloud.eu/feeds/nlp.atom.xml" rel="self"></link><id>https://marketplace.deep-hybrid-datacloud.eu/</id><updated>2023-05-15T12:31:19+02:00</updated><entry><title>Speech keywords classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-speech-to-text-tf.html" rel="alternate"></link><published>2019-07-31T00:00:00+02:00</published><updated>2023-05-15T12:31:19+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-07-31:/modules/deep-oc-speech-to-text-tf.html</id><summary type="html">&lt;p&gt;Train a speech classifier to classify audio files between different keywords.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-speech-to-text-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-speech-to-text-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool to train and evaluate a speech-to-text tool using deep neural networks. The network architecture is based on one of the tutorials provided by Tensorflow &lt;a href="https://www.tensorflow.org/tutorials/sequences/audio_recognition"&gt;1&lt;/a&gt;.
The architecture used in this tutorial is based on some described in the paper Convolutional Neural Networks for Small-footprint Keyword Spotting &lt;a href="https://static.googleusercontent.com/media/research.google.com/es//pubs/archive/43969.pdf"&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are lots of different approaches to building neural network models to work with audio including recurrent networks or dilated convolutions. This model is based on the kind of convolutional network that will feel very familiar to anyone who's worked with image recognition. That may seem surprising at first though, since audio is inherently a one-dimensional continuous signal across time, not a 2D spatial problem. We define a window of time we believe our spoken words should fit into, and converting the audio signal in that window into an image. This is done by grouping the incoming audio samples into short segments, just a few milliseconds long, and calculating the strength of the frequencies across a set of bands. Each set of frequency strengths from a segment is treated as a vector of numbers, and those vectors are arranged in time order to form a two-dimensional array. This array of values can then be treated like a single-channel image and is known as a spectrogram. These spectrograms will be the input for the training.
The container does not come with any pretrained model, it has to be trained first on a dataset to be used for prediction.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an audio files as input (or the url of an audio file) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-speech-to-text-tf/master/images/speech-to-text.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;</content><category term="tensorflow"></category></entry></feed>