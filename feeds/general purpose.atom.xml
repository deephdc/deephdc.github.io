<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>DEEP Open Catalog - general purpose</title><link href="https://marketplace.deep-hybrid-datacloud.eu/" rel="alternate"></link><link href="https://marketplace.deep-hybrid-datacloud.eu/feeds/general%20purpose.atom.xml" rel="self"></link><id>https://marketplace.deep-hybrid-datacloud.eu/</id><updated>2023-06-17T01:58:15+02:00</updated><entry><title>Object Detection and Classification with Pytorch</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-obj-detect-pytorch.html" rel="alternate"></link><published>2019-10-17T00:00:00+02:00</published><updated>2023-06-17T01:58:15+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-10-17:/modules/deep-oc-obj-detect-pytorch.html</id><summary type="html">&lt;p&gt;A trained Region Convolutional Neural Network (Faster RCNN) for object detection and classification.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-obj_detect_pytorch/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-obj_detect_pytorch/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool for object detection and classification using deep neural networks (Faster R-CNN 
ResNet-50 FPN Architecture [1]) that were already pretrained on the &lt;a href="http://cocodataset.org/#home"&gt;COCO Dataset&lt;/a&gt;. 
The code uses the Pytorch Library, more information can be found at &lt;a href="https://pytorch.org/docs/stabletorchvision/models.html#object-detection-instance-segmentation-and-person-keypoint-detection"&gt;Pytorch-Object-Detection&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an image as input and will return a JSON with 
the predictions that are greater than the probability threshold. Let's say you have an image of a cat 
and a dog together and the probability output was 50% a dog and 80% a cat,
if you set the threshold to 70%, the only detected object will be the cat, because its probability is grater than 70%.&lt;/p&gt;
&lt;p&gt;This module works on uploaded images and gives as output the rectangle coordinates x1,y1 and x2,y2 were the
classified object is located.
It also provides you the probability of the classified detected object.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit' width='30%', src='https://raw.githubusercontent.com/SilkeDH/obj_detect_pytorch/master/reports/figures/dog_broccoli.png'/&gt;&lt;/p&gt;
&lt;p&gt;The TRAINING method uses transfer learning and expects different parameters like learning rate, name of the clases, etc.
Transfer learning focuses on storing knowledge gained while solving one problem and applying it to a different but related
problem. To achieve it, the output layer of the pre-trained model is removed and a new one with the new number of outputs is
added. Only that new layer will be trained [1]. An example of transferred learning is provided an implemented in this module.&lt;/p&gt;
&lt;p&gt;The model requires a new dataset with the classes that are going to be classified and detected. In this case the Penn-Fudan
Database for Pedestrian Detection and Segmentation was used to detect pedestrians.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit' width='70%', src='https://raw.githubusercontent.com/SilkeDH/obj_detect_pytorch/master/reports/figures/pytorchobj.png'/&gt;&lt;/p&gt;
&lt;p&gt;To try this in the module, the two dataset folders (Images and Masks) must be placed in the obj_detect_pytorch/dataset/
folder. More information about the code, the Penn Fudan Dataset and the structuring of a custom dataset can be found at &lt;a href="https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"&gt;Torchvision Object Detection Finetuning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. &lt;a href="https://arxiv.org/abs/1506.01497"&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;
arXiv preprint (2016): 1506-01497.&lt;/p&gt;
&lt;p&gt;[2]: West Jeremy, Ventura Dan, Warnick Sean. Spring Research Presentation: A Theoretical Foundation for Inductive Transfer,
Brigham Young University, College of Physical and Mathematical Sciences. (2007).&lt;/p&gt;</content><category term="services"></category></entry><entry><title>Train an audio classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-audio-classification-tf.html" rel="alternate"></link><published>2019-09-01T00:00:00+02:00</published><updated>2023-06-17T01:58:11+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-09-01:/modules/deep-oc-audio-classification-tf.html</id><summary type="html">&lt;p&gt;Train your own audio classifier with your custom dataset. It comes also pretrained on the 527 AudioSet classes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-audio-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-audio-classification-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool to perform audio classification with Deep Learning.
It allows the user to classify their samples of audio as well as training their
own classifier for a custom problem.&lt;/p&gt;
&lt;p&gt;The classifier is currently pretrained on the 527 high-level classes from the
&lt;a href="https://research.google.com/audioset/"&gt;AudioSet&lt;/a&gt; dataset.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an audio file as input (or the url of a audio file) and will return a JSON with 
the top 5 predictions. Most audio file formats are supported (see &lt;a href="https://www.ffmpeg.org/"&gt;FFMPEG&lt;/a&gt; compatible formats).&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-audio-classification-tf/master/images/demo.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, Marvin Ritter,&lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45857.pdf"&gt;'Audio set: An ontology and human-labeled dataset for audio events'&lt;/a&gt;, IEEE ICASSP, 2017.&lt;/p&gt;
&lt;p&gt;Qiuqiang Kong, Yong Xu, Wenwu Wang, Mark D. Plumbley,&lt;a href="https://arxiv.org/pdf/1711.00927.pdf"&gt;'Audio Set classification with attention model: A probabilistic perspective.'&lt;/a&gt; arXiv preprint arXiv:1711.00927 (2017).&lt;/p&gt;
&lt;p&gt;Changsong Yu, Karim Said Barsim, Qiuqiang Kong, Bin Yang ,&lt;a href="https://arxiv.org/pdf/1803.02353.pdf"&gt;'Multi-level Attention Model for Weakly Supervised Audio Classification.'&lt;/a&gt; arXiv preprint arXiv:1803.02353 (2018).&lt;/p&gt;
&lt;p&gt;S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et  al., &lt;a href="https://arxiv.org/pdf/1609.09430.pdf"&gt;'CNN architectures for large-scale audio classification,'&lt;/a&gt; arXiv preprint arXiv:1609.09430, 2016.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Speech keywords classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-speech-to-text-tf.html" rel="alternate"></link><published>2019-07-31T00:00:00+02:00</published><updated>2023-06-17T01:58:09+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-07-31:/modules/deep-oc-speech-to-text-tf.html</id><summary type="html">&lt;p&gt;Train a speech classifier to classify audio files between different keywords.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-speech-to-text-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-speech-to-text-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool to train and evaluate a speech-to-text tool using deep neural networks. The network architecture is based on one of the tutorials provided by Tensorflow &lt;a href="https://www.tensorflow.org/tutorials/sequences/audio_recognition"&gt;1&lt;/a&gt;.
The architecture used in this tutorial is based on some described in the paper Convolutional Neural Networks for Small-footprint Keyword Spotting &lt;a href="https://static.googleusercontent.com/media/research.google.com/es//pubs/archive/43969.pdf"&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are lots of different approaches to building neural network models to work with audio including recurrent networks or dilated convolutions. This model is based on the kind of convolutional network that will feel very familiar to anyone who's worked with image recognition. That may seem surprising at first though, since audio is inherently a one-dimensional continuous signal across time, not a 2D spatial problem. We define a window of time we believe our spoken words should fit into, and converting the audio signal in that window into an image. This is done by grouping the incoming audio samples into short segments, just a few milliseconds long, and calculating the strength of the frequencies across a set of bands. Each set of frequency strengths from a segment is treated as a vector of numbers, and those vectors are arranged in time order to form a two-dimensional array. This array of values can then be treated like a single-channel image and is known as a spectrogram. These spectrograms will be the input for the training.
The container does not come with any pretrained model, it has to be trained first on a dataset to be used for prediction.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an audio files as input (or the url of an audio file) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-speech-to-text-tf/master/images/speech-to-text.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Train an image classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-image-classification-tf.html" rel="alternate"></link><published>2019-01-01T00:00:00+01:00</published><updated>2023-06-17T01:58:02+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-01-01:/modules/deep-oc-image-classification-tf.html</id><summary type="html">&lt;p&gt;Train your own image classifier with your custom dataset. It comes also pretrained on the 1K ImageNet classes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-image-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-image-classification-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The deep learning revolution has brought significant advances in a number of fields [1], primarily linked to
image and speech recognition. The standardization of image classification tasks like the &lt;a href="http://www.image-net.org/challenges/LSVRC/"&gt;ImageNet Large Scale
Visual Recognition Challenge&lt;/a&gt; [2] has resulted in a reliable way to
compare top performing architectures.&lt;/p&gt;
&lt;p&gt;This Docker container contains the tools to train an image classifier on your personal dataset. It is a highly
customizable tool  that let's you choose between tens of different &lt;a href="https://github.com/keras-team/keras-applications"&gt;top performing architectures&lt;/a&gt;
and training parameters.&lt;/p&gt;
&lt;p&gt;The container also comes with a pretrained general-purpose image classifier trained on ImageNet.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an RGB image as input (or the url of an RGB image) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-image-classification-tf/master/images/imagenet.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1]: Yann LeCun, Yoshua Bengio, and Geofrey Hinton. &lt;a href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf"&gt;Deep learning&lt;/a&gt;. Nature, 521(7553):436-444, May 2015.&lt;/p&gt;
&lt;p&gt;[2]: Olga Russakovsky et al. &lt;a href="https://arxiv.org/abs/1409.0575"&gt;ImageNet Large Scale Visual Recognition Challenge&lt;/a&gt;. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.&lt;/p&gt;</content><category term="tensorflow"></category></entry></feed>