<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>DEEP Open Catalog - audio</title><link href="https://marketplace.deep-hybrid-datacloud.eu/" rel="alternate"></link><link href="https://marketplace.deep-hybrid-datacloud.eu/feeds/audio.atom.xml" rel="self"></link><id>https://marketplace.deep-hybrid-datacloud.eu/</id><updated>2023-10-04T13:19:45+02:00</updated><entry><title>Train an audio classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-audio-classification-tf.html" rel="alternate"></link><published>2019-09-01T00:00:00+02:00</published><updated>2023-10-04T13:19:45+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-09-01:/modules/deep-oc-audio-classification-tf.html</id><summary type="html">&lt;p&gt;Train your own audio classifier with your custom dataset. It comes also pretrained on the 527 AudioSet classes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-audio-classification-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-audio-classification-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool to perform audio classification with Deep Learning.
It allows the user to classify their samples of audio as well as training their
own classifier for a custom problem.&lt;/p&gt;
&lt;p&gt;The classifier is currently pretrained on the 527 high-level classes from the
&lt;a href="https://research.google.com/audioset/"&gt;AudioSet&lt;/a&gt; dataset.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an audio file as input (or the url of a audio file) and will return a JSON with 
the top 5 predictions. Most audio file formats are supported (see &lt;a href="https://www.ffmpeg.org/"&gt;FFMPEG&lt;/a&gt; compatible formats).&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-audio-classification-tf/master/images/demo.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, Marvin Ritter,&lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45857.pdf"&gt;'Audio set: An ontology and human-labeled dataset for audio events'&lt;/a&gt;, IEEE ICASSP, 2017.&lt;/p&gt;
&lt;p&gt;Qiuqiang Kong, Yong Xu, Wenwu Wang, Mark D. Plumbley,&lt;a href="https://arxiv.org/pdf/1711.00927.pdf"&gt;'Audio Set classification with attention model: A probabilistic perspective.'&lt;/a&gt; arXiv preprint arXiv:1711.00927 (2017).&lt;/p&gt;
&lt;p&gt;Changsong Yu, Karim Said Barsim, Qiuqiang Kong, Bin Yang ,&lt;a href="https://arxiv.org/pdf/1803.02353.pdf"&gt;'Multi-level Attention Model for Weakly Supervised Audio Classification.'&lt;/a&gt; arXiv preprint arXiv:1803.02353 (2018).&lt;/p&gt;
&lt;p&gt;S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et  al., &lt;a href="https://arxiv.org/pdf/1609.09430.pdf"&gt;'CNN architectures for large-scale audio classification,'&lt;/a&gt; arXiv preprint arXiv:1609.09430, 2016.&lt;/p&gt;</content><category term="tensorflow"></category></entry><entry><title>Speech keywords classifier</title><link href="https://marketplace.deep-hybrid-datacloud.eu/modules/deep-oc-speech-to-text-tf.html" rel="alternate"></link><published>2019-07-31T00:00:00+02:00</published><updated>2023-10-04T13:19:43+02:00</updated><author><name>DEEP-Hybrid-DataCloud Consortium</name></author><id>tag:marketplace.deep-hybrid-datacloud.eu,2019-07-31:/modules/deep-oc-speech-to-text-tf.html</id><summary type="html">&lt;p&gt;Train a speech classifier to classify audio files between different keywords.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/DEEP-OC-org/job/DEEP-OC-speech-to-text-tf/job/master"&gt;&lt;img alt="Build Status" src="https://jenkins.indigo-datacloud.eu/buildStatus/icon?job=Pipeline-as-code/DEEP-OC-org/DEEP-OC-speech-to-text-tf/master"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a plug-and-play tool to train and evaluate a speech-to-text tool using deep neural networks. The network architecture is based on one of the tutorials provided by Tensorflow &lt;a href="https://www.tensorflow.org/tutorials/sequences/audio_recognition"&gt;1&lt;/a&gt;.
The architecture used in this tutorial is based on some described in the paper Convolutional Neural Networks for Small-footprint Keyword Spotting &lt;a href="https://static.googleusercontent.com/media/research.google.com/es//pubs/archive/43969.pdf"&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are lots of different approaches to building neural network models to work with audio including recurrent networks or dilated convolutions. This model is based on the kind of convolutional network that will feel very familiar to anyone who's worked with image recognition. That may seem surprising at first though, since audio is inherently a one-dimensional continuous signal across time, not a 2D spatial problem. We define a window of time we believe our spoken words should fit into, and converting the audio signal in that window into an image. This is done by grouping the incoming audio samples into short segments, just a few milliseconds long, and calculating the strength of the frequencies across a set of bands. Each set of frequency strengths from a segment is treated as a vector of numbers, and those vectors are arranged in time order to form a two-dimensional array. This array of values can then be treated like a single-channel image and is known as a spectrogram. These spectrograms will be the input for the training.
The container does not come with any pretrained model, it has to be trained first on a dataset to be used for prediction.&lt;/p&gt;
&lt;p&gt;The PREDICT method expects an audio files as input (or the url of an audio file) and will return a JSON with 
the top 5 predictions.&lt;/p&gt;
&lt;p&gt;&lt;img class='fit', src='https://raw.githubusercontent.com/deephdc/DEEP-OC-speech-to-text-tf/master/images/speech-to-text.png'/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;</content><category term="tensorflow"></category></entry></feed>